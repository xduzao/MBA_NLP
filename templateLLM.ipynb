{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8a877a",
   "metadata": {},
   "source": [
    "\n",
    "# Case QuantumFinance — Classificador de Chamados (Template)\n",
    "\n",
    "**Objetivo:** construir e avaliar um classificador de assuntos para chamados da QuantumFinance usando **PLN**, **vetorização (n‑grama + métrica)** e **modelo supervisionado**, alcançando **F1 (weighted) ≥ 0.75**.  \n",
    "Além do modelo clássico (BoW/TF‑IDF), incluir pelo menos **uma abordagem com Embeddings** (**Word2Vec** e/ou **Sentence Transformers/LLM**).  \n",
    "Separar **75% treino / 25% teste** com `random_state=42` e, ao final, **consolidar o *pipeline do modelo campeão***.\n",
    "\n",
    "> Este notebook segue as etapas das aulas 1–6 (pré‑processamento, vetorização, modelos clássicos, embeddings e LLMs) e automatiza a comparação de modelos, selecionando o campeão e exibindo seu pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e645a0",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Instalação de dependências (execute apenas uma vez por ambiente)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a240972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Se estiver em ambiente sem internet, comente as linhas de instalação.\n",
    "# !pip -q install scikit-learn pandas numpy matplotlib nltk gensim unidecode\n",
    "# !pip -q install sentence-transformers # (para embeddings LLM)\n",
    "# !pip -q install spacy\n",
    "# !python -m spacy download pt_core_news_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11854387",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports e Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, math, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from unidecode import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "# Gensim (Word2Vec)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sentence Transformers (LLM Embeddings) - requer internet para baixar no primeiro uso\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _HAS_ST = True\n",
    "except Exception as _e:\n",
    "    _HAS_ST = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# Stopwords PT\n",
    "STOP_PT = set(stopwords.words('portuguese'))\n",
    "\n",
    "print(\"Setup concluído. SentenceTransformer disponível?\", _HAS_ST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45198b9b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Carregamento dos dados e EDA rápida\n",
    "\n",
    "> Dataset oficial: `https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv`  \n",
    "> **Dica:** se o ambiente bloquear internet, baixe o CSV e aponte `DATA_PATH` para o arquivo local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Caminho dos dados — ajuste se necessário\n",
    "DATA_PATH = \"tickets_reclamacoes_classificados.csv\"  # ou use a URL direta se o ambiente permitir\n",
    "\n",
    "# Leitura\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "except Exception as e:\n",
    "    print(\"Falha ao ler o CSV no caminho atual. Tente baixar o arquivo localmente ou usar a URL.\")\n",
    "    print(\"Erro:\", e)\n",
    "    # Tente URL direta\n",
    "    try:\n",
    "        df = pd.read_csv(\"https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv\")\n",
    "        print(\"Lido via URL com sucesso.\")\n",
    "    except Exception as e2:\n",
    "        raise RuntimeError(\"Não foi possível carregar o dataset automaticamente. Baixe o CSV e ajuste DATA_PATH.\") from e2\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Inferir nomes de colunas esperadas\n",
    "# Pressupõe colunas como: 'texto' (ou 'mensagem'/'descricao' etc.) e 'assunto' (rótulo)\n",
    "col_text_candidates = [c for c in df.columns if c.lower() in (\"texto\",\"mensagem\",\"descricao\",\"descricao_texto\",\"conteudo\")]\n",
    "col_label_candidates = [c for c in df.columns if c.lower() in (\"assunto\",\"categoria\",\"classe\",\"label\",\"rotulo\")]\n",
    "\n",
    "if not col_text_candidates or not col_label_candidates:\n",
    "    print(\"\\nATENÇÃO: Não identifiquei colunas padrão. Ajuste os nomes abaixo.\")\n",
    "    TEXT_COL = \"texto\"\n",
    "    LABEL_COL = \"assunto\"\n",
    "else:\n",
    "    TEXT_COL = col_text_candidates[0]\n",
    "    LABEL_COL = col_label_candidates[0]\n",
    "\n",
    "print(f\"Coluna de texto: {TEXT_COL} | Coluna de rótulo: {LABEL_COL}\")\n",
    "\n",
    "# Limpeza básica de nulos\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL])\n",
    "after = len(df)\n",
    "print(f\"Removidos {before-after} registros com nulos em {TEXT_COL}/{LABEL_COL}.\")\n",
    "\n",
    "# Distribuição de classes\n",
    "print(\"\\nDistribuição de classes:\")\n",
    "print(df[LABEL_COL].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2a112",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Pré-processamento (Aulas 1–2)\n",
    "\n",
    "- **lowercase, remoção de pontuação**, **remover acentos** (opcional, com `unidecode`),  \n",
    "- **tokenização**, **remoção de stopwords**, **stem em PT (RSLP)** ou **lematização** via spaCy (opcional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STEMMER = RSLPStemmer()\n",
    "\n",
    "def basic_normalize(text:str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower().strip()\n",
    "    # Remover acentuação (opcional — comente se não quiser)\n",
    "    text = unidecode(text)\n",
    "    # Remover caracteres não alfanuméricos básicos (deixe espaço)\n",
    "    text = re.sub(r\"[^a-z0-9áéíóúãõç\\s]\", \" \", text)  # após unidecode, acentos já se foram; mantido para caso desative\n",
    "    # Colapsar espaços\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_pt(text:str):\n",
    "    return word_tokenize(text, language='portuguese')\n",
    "\n",
    "def preprocess_tokens(tokens, remove_stop=True, do_stem=True):\n",
    "    out = []\n",
    "    for w in tokens:\n",
    "        if remove_stop and w in STOP_PT:\n",
    "            continue\n",
    "        if do_stem:\n",
    "            try:\n",
    "                w = STEMMER.stem(w)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if w:\n",
    "            out.append(w)\n",
    "    return out\n",
    "\n",
    "def preprocess_text(text:str, remove_stop=True, do_stem=True, join=True):\n",
    "    text = basic_normalize(text)\n",
    "    tokens = tokenize_pt(text)\n",
    "    tokens = preprocess_tokens(tokens, remove_stop=remove_stop, do_stem=do_stem)\n",
    "    return \" \".join(tokens) if join else tokens\n",
    "\n",
    "# Exemplo rápido\n",
    "sample = df[TEXT_COL].iloc[0]\n",
    "print(\"Original:\", sample[:200], \"...\")\n",
    "print(\"Processado:\", preprocess_text(sample)[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281671c",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Split (75% treino / 25% teste, `random_state=42`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_raw = df[TEXT_COL].astype(str).tolist()\n",
    "y = df[LABEL_COL].astype(str).tolist()\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Pré-processamento aplicado antes de BoW/TF-IDF (opcional; também testaremos sem)\n",
    "X_train_pp = [preprocess_text(t) for t in X_train_raw]\n",
    "X_test_pp  = [preprocess_text(t) for t in X_test_raw]\n",
    "\n",
    "len(X_train_pp), len(X_test_pp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927f7c7",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Modelos clássicos (BoW / TF‑IDF) — *Baselines & Grid simples*\n",
    "\n",
    "Testes com:\n",
    "- **CountVectorizer** e **TfidfVectorizer**\n",
    "- n‑gramas `(1,1)` e `(1,2)`\n",
    "- **LogisticRegression**, **LinearSVC**, **MultinomialNB**, **RandomForest**\n",
    "- Com **texto bruto** e com **texto pré‑processado**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_pipeline(vectorizer, clf, Xtr, Xte, ytr, yte, name:str):\n",
    "    pipe = Pipeline([\n",
    "        (\"vec\", vectorizer),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    ypred = pipe.predict(Xte)\n",
    "    f1w = f1_score(yte, ypred, average='weighted')\n",
    "    return {\"name\": name, \"pipeline\": pipe, \"f1_weighted\": f1w}\n",
    "\n",
    "results = []\n",
    "\n",
    "vectorizers = [\n",
    "    (\"count_1\", CountVectorizer(ngram_range=(1,1))),\n",
    "    (\"count_12\", CountVectorizer(ngram_range=(1,2))),\n",
    "    (\"tfidf_1\", TfidfVectorizer(ngram_range=(1,1))),\n",
    "    (\"tfidf_12\", TfidfVectorizer(ngram_range=(1,2))),\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    (\"logreg\", LogisticRegression(max_iter=2000, n_jobs=None)),\n",
    "    (\"linsvc\", LinearSVC()),\n",
    "    (\"mnb\", MultinomialNB()),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "datasets = [(\"raw\", X_train_raw, X_test_raw), (\"pp\", X_train_pp, X_test_pp)]\n",
    "\n",
    "for ds_name, Xtr, Xte in datasets:\n",
    "    for v_name, vec in vectorizers:\n",
    "        for c_name, clf in classifiers:\n",
    "            name = f\"{ds_name}__{v_name}__{c_name}\"\n",
    "            try:\n",
    "                res = eval_pipeline(vec, clf, Xtr, Xte, y_train, y_test, name)\n",
    "                results.append(res)\n",
    "                print(f\"{name}: F1w={res['f1_weighted']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Falhou {name}: {e}\")\n",
    "\n",
    "df_results_baseline = pd.DataFrame([{\"name\": r[\"name\"], \"f1_weighted\": r[\"f1_weighted\"]} for r in results]).sort_values(\"f1_weighted\", ascending=False)\n",
    "df_results_baseline.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31091474",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Embeddings — **Word2Vec** (média de vetores por documento)\n",
    "\n",
    "- Treina Skip‑gram (`sg=1`) no corpus pré‑processado.\n",
    "- Vetor de documento = **média** dos vetores de tokens presentes.\n",
    "- Classificador: **LogisticRegression** (padrão comparável).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treinar Word2Vec no corpus pré-processado\n",
    "tokens_train = [preprocess_text(t, join=False) for t in X_train_raw]\n",
    "tokens_test  = [preprocess_text(t, join=False) for t in X_test_raw]\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=tokens_train,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "def sent2vec(tokens, model):\n",
    "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size, dtype=float)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "Xtr_w2v = np.vstack([sent2vec(tok, w2v) for tok in tokens_train])\n",
    "Xte_w2v = np.vstack([sent2vec(tok, w2v) for tok in tokens_test])\n",
    "\n",
    "clf_w2v = LogisticRegression(max_iter=1000)\n",
    "clf_w2v.fit(Xtr_w2v, y_train)\n",
    "yp_w2v = clf_w2v.predict(Xte_w2v)\n",
    "f1_w2v = f1_score(y_test, yp_w2v, average='weighted')\n",
    "print(f\"Word2Vec + LogReg -> F1 (weighted): {f1_w2v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9f56f",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Embeddings — **Sentence Transformers (LLM)**\n",
    "\n",
    "Modelo recomendado (multilíngue):  \n",
    "- `'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'`  \n",
    "ou `'sentence-transformers/distiluse-base-multilingual-cased-v2'`.\n",
    "\n",
    "> **Observação:** na primeira execução, o modelo será baixado da internet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st_f1 = None\n",
    "st_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "if _HAS_ST:\n",
    "    try:\n",
    "        st_model = SentenceTransformer(st_model_name)\n",
    "        Xtr_st = st_model.encode(X_train_raw, show_progress_bar=True)\n",
    "        Xte_st = st_model.encode(X_test_raw,  show_progress_bar=True)\n",
    "\n",
    "        clf_st = LogisticRegression(max_iter=2000)\n",
    "        clf_st.fit(Xtr_st, y_train)\n",
    "        yp_st = clf_st.predict(Xte_st)\n",
    "        st_f1 = f1_score(y_test, yp_st, average='weighted')\n",
    "        print(f\"SentenceTransformer + LogReg -> F1 (weighted): {st_f1:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Falhou SentenceTransformer:\", e)\n",
    "else:\n",
    "    print(\"SentenceTransformer não disponível neste ambiente. Pule esta célula ou instale as dependências.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbded3c",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Comparação e seleção do **Modelo Campeão**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agregar resultados\n",
    "rows = df_results_baseline.copy()\n",
    "rows[\"source\"] = \"baseline\"\n",
    "\n",
    "# Adicionar W2V\n",
    "rows = pd.concat([\n",
    "    rows,\n",
    "    pd.DataFrame([{\"name\":\"w2v__logreg\", \"f1_weighted\": float(f1_w2v), \"source\":\"w2v\"}])\n",
    "], ignore_index=True)\n",
    "\n",
    "# Adicionar ST se existir\n",
    "if st_f1 is not None:\n",
    "    rows = pd.concat([\n",
    "        rows,\n",
    "        pd.DataFrame([{\"name\":\"st__logreg\", \"f1_weighted\": float(st_f1), \"source\":\"st\"}])\n",
    "    ], ignore_index=True)\n",
    "\n",
    "rows = rows.sort_values(\"f1_weighted\", ascending=False).reset_index(drop=True)\n",
    "rows.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e967d9",
   "metadata": {},
   "source": [
    "\n",
    "## 9. **Pipeline do Modelo Campeão** (limpo e pronto para inferência)\n",
    "\n",
    "Abaixo criamos uma **única versão de pipeline** correspondente ao melhor resultado acima.  \n",
    "O código identifica se o campeão foi **baseline (TF‑IDF/BoW)**, **Word2Vec** ou **SentenceTransformer**, e então:\n",
    "- instancia o *transformer* adequado,\n",
    "- re‑treina no *train*,\n",
    "- avalia no *test* com relatório,\n",
    "- expõe função `predict_chamado(textos)` para uso direto.\n",
    "\n",
    "> Se preferir fixar manualmente o campeão, altere `champion_name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "champion_name = rows.iloc[0][\"name\"]\n",
    "print(\"Modelo campeão detectado:\", champion_name)\n",
    "\n",
    "class STVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer que converte textos em embeddings usando SentenceTransformer.\"\"\"\n",
    "    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "    def fit(self, X, y=None):\n",
    "        if self.model is None:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return self.model.encode(list(X), show_progress_bar=False)\n",
    "\n",
    "class W2VVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer que cria embeddings médios Word2Vec.\"\"\"\n",
    "    def __init__(self, vector_size=100, window=5, min_count=2, sg=1, seed=RANDOM_STATE):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "    def fit(self, X, y=None):\n",
    "        tokens = [preprocess_text(t, join=False) for t in X]\n",
    "        self.model = Word2Vec(\n",
    "            sentences=tokens,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            sg=self.sg,\n",
    "            workers=4,\n",
    "            seed=self.seed\n",
    "        )\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        tokens_list = [preprocess_text(t, join=False) for t in X]\n",
    "        def sent2vec(tokens):\n",
    "            vecs = [self.model.wv[w] for w in tokens if w in self.model.wv]\n",
    "            if not vecs:\n",
    "                return np.zeros(self.model.vector_size, dtype=float)\n",
    "            return np.mean(vecs, axis=0)\n",
    "        arr = np.vstack([sent2vec(toks) for toks in tokens_list])\n",
    "        return arr\n",
    "\n",
    "# Mapeamento para reconstruir pipeline campeão\n",
    "def build_champion(name:str):\n",
    "    if name.startswith(\"st__\"):\n",
    "        if not _HAS_ST:\n",
    "            raise RuntimeError(\"SentenceTransformer não disponível para reconstruir o campeão.\")\n",
    "        pipe = Pipeline([\n",
    "            (\"emb\", STVectorizer()),\n",
    "            (\"clf\", LogisticRegression(max_iter=2000))\n",
    "        ])\n",
    "        pipe.fit(X_train_raw, y_train)\n",
    "        return pipe, \"SentenceTransformer + LogReg\"\n",
    "    elif name.startswith(\"w2v__\"):\n",
    "        pipe = Pipeline([\n",
    "            (\"emb\", W2VVectorizer()),\n",
    "            (\"clf\", LogisticRegression(max_iter=1000))\n",
    "        ])\n",
    "        pipe.fit(X_train_raw, y_train)\n",
    "        return pipe, \"Word2Vec + LogReg\"\n",
    "    else:\n",
    "        # Baselines: descobrir configuração aproximada pelo sufixo\n",
    "        # Formato esperado: ds__vec__clf (ex.: \"pp__tfidf_12__logreg\")\n",
    "        parts = name.split(\"__\")\n",
    "        ds, vec_tag, clf_tag = parts[0], parts[1], parts[2]\n",
    "\n",
    "        # Escolher dataset\n",
    "        Xtr = X_train_pp if ds == \"pp\" else X_train_raw\n",
    "\n",
    "        # Escolher vetorizador\n",
    "        if vec_tag.startswith(\"tfidf\"):\n",
    "            ngram = (1,2) if \"12\" in vec_tag else (1,1)\n",
    "            vec = TfidfVectorizer(ngram_range=ngram)\n",
    "        elif vec_tag.startswith(\"count\"):\n",
    "            ngram = (1,2) if \"12\" in vec_tag else (1,1)\n",
    "            vec = CountVectorizer(ngram_range=ngram)\n",
    "        else:\n",
    "            raise ValueError(\"Vec tag desconhecido:\", vec_tag)\n",
    "\n",
    "        # Escolher classificador\n",
    "        if clf_tag == \"logreg\":\n",
    "            clf = LogisticRegression(max_iter=2000)\n",
    "        elif clf_tag == \"linsvc\":\n",
    "            clf = LinearSVC()\n",
    "        elif clf_tag == \"mnb\":\n",
    "            clf = MultinomialNB()\n",
    "        elif clf_tag == \"rf\":\n",
    "            clf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            raise ValueError(\"Classifier tag desconhecido:\", clf_tag)\n",
    "\n",
    "        pipe = Pipeline([(\"vec\", vec), (\"clf\", clf)])\n",
    "        pipe.fit(Xtr, y_train)\n",
    "        return pipe, f\"{vec.__class__.__name__} + {clf.__class__.__name__} ({'PP' if ds=='pp' else 'RAW'})\"\n",
    "\n",
    "champion_pipe, champion_desc = build_champion(champion_name)\n",
    "\n",
    "# Avaliação no conjunto de teste apropriado ao pipeline\n",
    "def choose_test_input(pipe, name):\n",
    "    # Para baselines usamos RAW ou PP conforme detectado no nome\n",
    "    if name.startswith(\"st__\") or name.startswith(\"w2v__\"):\n",
    "        return X_test_raw\n",
    "    else:\n",
    "        ds = name.split(\"__\")[0]\n",
    "        return X_test_pp if ds == \"pp\" else X_test_raw\n",
    "\n",
    "Xte_input = choose_test_input(champion_pipe, champion_name)\n",
    "ypred = champion_pipe.predict(Xte_input)\n",
    "f1w = f1_score(y_test, ypred, average='weighted')\n",
    "print(\"\\n== CAMPEÃO ==\")\n",
    "print(\"Descrição:\", champion_desc)\n",
    "print(f\"F1 (weighted) no teste: {f1w:.4f}\\n\")\n",
    "print(classification_report(y_test, ypred))\n",
    "\n",
    "cm = confusion_matrix(y_test, ypred, labels=sorted(list(set(y))))\n",
    "print(\"Matriz de confusão (ordem de labels alfabética):\")\n",
    "print(cm)\n",
    "\n",
    "def predict_chamado(textos):\n",
    "    \"\"\"Inferência direta no pipeline campeão (lista de strings).\"\"\"\n",
    "    if isinstance(textos, str):\n",
    "        textos = [textos]\n",
    "    return champion_pipe.predict(textos)\n",
    "\n",
    "# Exemplo de inferência:\n",
    "# predict_chamado([\"Não consigo acessar minha conta desde ontem\", \"Cobrança duplicada no cartão\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817286b0",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Conclusões e próximos passos\n",
    "\n",
    "- **Resumo das técnicas usadas:** pré‑processamento (lower, regex/limpeza, stopwords, stem/lemma), BoW/TF‑IDF, Word2Vec, SentenceTransformer.\n",
    "- **Comparação de F1 (weighted):** vide tabela na seção 8 para justificar a escolha do campeão.\n",
    "- **Modelo campeão:** ver seção 9 — pipeline único, limpo e pronto para re‑treino/inferência.\n",
    "- **Melhorias possíveis:**\n",
    "  - *Tuning* de hiperparâmetros (C, n_estimators, etc.).\n",
    "  - Balanceamento de classes (class_weight, amostragem estratificada, técnicas de reamostragem).\n",
    "  - Lematização com spaCy (PT) + POS‑tag (filtragem por substantivos/verbos).\n",
    "  - Embeddings mais robustos (ex.: `all-MiniLM-L6-v2` multilíngue; ou BERTimbau no HuggingFace).\n",
    "  - Validação cruzada e *nested CV* para evitar *overfitting*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
